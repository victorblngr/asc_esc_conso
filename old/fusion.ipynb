{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41a795c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4747708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup basic logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25bf5b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base path\n",
    "BASE_FOLDER = \"points_marquants\"\n",
    "# Ensure the base folder path is correct relative to the script execution directory\n",
    "folder_path = os.path.join(os.getcwd(), BASE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4035d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to keep and their new names\n",
    "COLUMNS_TO_KEEP = [\n",
    "    \"DATE Début\",\n",
    "    \"HEURE Début\",\n",
    "    \"DATE Fin\",\n",
    "    \"HEURE Fin\",\n",
    "    \"LIGNE\",\n",
    "    \"STATION\",\n",
    "    \"N° EQUIP.\",\n",
    "    \"COMMENTAIRE\",\n",
    "    \"Motifs\",\n",
    "]\n",
    "COLUMN_RENAME_MAP = {\n",
    "    \"DATE Début\": \"date_debut_panne\",\n",
    "    \"HEURE Début\": \"heure_debut_panne\",\n",
    "    \"DATE Fin\": \"date_fin_panne\",\n",
    "    \"HEURE Fin\": \"heure_fin_panne\",\n",
    "    \"LIGNE\": \"ligne\",\n",
    "    \"STATION\": \"station\",\n",
    "    \"N° EQUIP.\": \"num_equip\",\n",
    "    \"COMMENTAIRE\": \"commentaire\",\n",
    "    \"Motifs\": \"motifs\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "071595fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts various time representations to HH:MM string format\n",
    "def convert_time_format(time_val):\n",
    "    \"\"\"Converts various time representations to HH:MM string format.\"\"\"\n",
    "    if pd.isna(time_val):\n",
    "        return None\n",
    "    if isinstance(\n",
    "        time_val, pd.Timestamp\n",
    "    ):  # Handle datetime.time objects if read_excel parses them\n",
    "        return time_val.strftime(\"%H:%M\")\n",
    "    time_str = str(time_val)\n",
    "    # Handle float times like 0.5 for noon\n",
    "    try:\n",
    "        float_time = float(time_str)\n",
    "        if 0 <= float_time <= 1:\n",
    "            # Convert fractional day to HH:MM\n",
    "            total_minutes = int(float_time * 24 * 60)\n",
    "            hours = total_minutes // 60\n",
    "            minutes = total_minutes % 60\n",
    "            return f\"{hours:02d}:{minutes:02d}\"\n",
    "    except ValueError:\n",
    "        pass  # It's not a simple float, proceed with string parsing\n",
    "\n",
    "    # Handle string formats like '9h', '14h30', '17H'\n",
    "    time_str = time_str.replace(\"h\", \":\").replace(\"H\", \":\")\n",
    "    if \":\" in time_str:\n",
    "        parts = time_str.split(\":\")\n",
    "        hour = parts[0].strip()\n",
    "        minute = parts[1].strip() if len(parts) > 1 and parts[1].strip() else \"00\"\n",
    "        # Basic validation/cleanup\n",
    "        try:\n",
    "            hour_int = int(hour)\n",
    "            minute_int = int(minute)\n",
    "            if 0 <= hour_int <= 23 and 0 <= minute_int <= 59:\n",
    "                return f\"{hour_int:02d}:{minute_int:02d}\"\n",
    "        except ValueError:\n",
    "            pass  # Invalid format\n",
    "    # Handle potential integer times (less likely but possible)\n",
    "    try:\n",
    "        int_time = int(time_str)\n",
    "        if 0 <= int_time <= 23:  # Assume it's just the hour\n",
    "            return f\"{int_time:02d}:00\"\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    logging.warning(f\"Could not parse time format: {time_val}. Returning None.\")\n",
    "    return None  # Return None if format is unrecognized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b023014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates duration in hours and days between start and end date/time\n",
    "def calculate_duration(row):\n",
    "    \"\"\"Calculates duration in hours and days between start and end date/time.\"\"\"\n",
    "    if (\n",
    "        pd.isna(row[\"date_debut_panne\"])\n",
    "        or pd.isna(row[\"date_fin_panne\"])\n",
    "        or pd.isna(row[\"heure_debut_panne\"])\n",
    "        or pd.isna(row[\"heure_fin_panne\"])\n",
    "    ):\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        # Combine date (already datetime) and time string\n",
    "        start_str = (\n",
    "            f\"{row['date_debut_panne'].strftime('%Y-%m-%d')} {row['heure_debut_panne']}\"\n",
    "        )\n",
    "        end_str = (\n",
    "            f\"{row['date_fin_panne'].strftime('%Y-%m-%d')} {row['heure_fin_panne']}\"\n",
    "        )\n",
    "\n",
    "        start_dt = pd.to_datetime(start_str, format=\"%Y-%m-%d %H:%M\", errors=\"coerce\")\n",
    "        end_dt = pd.to_datetime(end_str, format=\"%Y-%m-%d %H:%M\", errors=\"coerce\")\n",
    "\n",
    "        if pd.isna(start_dt) or pd.isna(end_dt):\n",
    "            logging.warning(\n",
    "                f\"Could not parse combined datetime for row: {row.name}. Start: '{start_str}', End: '{end_str}'\"\n",
    "            )\n",
    "            return None, None\n",
    "\n",
    "        duration_timedelta = end_dt - start_dt\n",
    "        duration_hours = duration_timedelta.total_seconds() / 3600\n",
    "        duration_days = duration_hours / 24\n",
    "        return duration_hours, duration_days\n",
    "    except Exception as e:\n",
    "        logging.error(\n",
    "            f\"Error calculating duration for row {row.name}: {e}. Data: {row['date_debut_panne']}, {row['heure_debut_panne']}, {row['date_fin_panne']}, {row['heure_fin_panne']}\"\n",
    "        )\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c16dbc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_maintenance_file(\n",
    "    input_filename, output_base_name, skiprows=0, sheet_name=0\n",
    "):\n",
    "    \"\"\"Reads, cleans, transforms, and saves maintenance data from an Excel file.\"\"\"\n",
    "    input_path = os.path.join(folder_path, input_filename)\n",
    "    output_csv_path = os.path.join(folder_path, f\"{output_base_name}.csv\")\n",
    "    output_excel_path = os.path.join(folder_path, f\"{output_base_name}.xlsx\")\n",
    "\n",
    "    logging.info(f\"Processing file: {input_filename}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(input_path, skiprows=skiprows, sheet_name=sheet_name)\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Input file not found: {input_path}. Skipping.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading Excel file {input_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- Data Cleaning and Transformation ---\n",
    "    # Ensure required columns exist\n",
    "    missing_cols = [col for col in COLUMNS_TO_KEEP if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        logging.error(\n",
    "            f\"Missing required columns in {input_filename}: {missing_cols}. Skipping.\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    df_processed = df[\n",
    "        COLUMNS_TO_KEEP\n",
    "    ].copy()  # Select and copy to avoid SettingWithCopyWarning\n",
    "    df_processed.rename(columns=COLUMN_RENAME_MAP, inplace=True)\n",
    "\n",
    "    # Convert dates first (errors='coerce' handles unparseable dates -> NaT)\n",
    "    # Keep them as datetime objects for now\n",
    "    df_processed[\"date_debut_panne\"] = pd.to_datetime(\n",
    "        df_processed[\"date_debut_panne\"], errors=\"coerce\"\n",
    "    )\n",
    "    df_processed[\"date_fin_panne\"] = pd.to_datetime(\n",
    "        df_processed[\"date_fin_panne\"], errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # Convert times using the helper function\n",
    "    df_processed[\"heure_debut_panne\"] = df_processed[\"heure_debut_panne\"].apply(\n",
    "        convert_time_format\n",
    "    )\n",
    "    df_processed[\"heure_fin_panne\"] = df_processed[\"heure_fin_panne\"].apply(\n",
    "        convert_time_format\n",
    "    )\n",
    "\n",
    "    # Handle potential NaNs introduced by conversions before proceeding\n",
    "    df_processed.dropna(\n",
    "        subset=[\"date_debut_panne\", \"num_equip\"], inplace=True\n",
    "    )  # Essential columns\n",
    "\n",
    "    # Create 'type_equipement'\n",
    "    # Ensure 'num_equip' is string type before using .str accessor\n",
    "    df_processed[\"num_equip\"] = df_processed[\"num_equip\"].astype(str)\n",
    "    df_processed[\"type_equipement\"] = (\n",
    "        df_processed[\"num_equip\"]\n",
    "        .str.startswith(\"Asc\")\n",
    "        .map({True: \"ascenseur\", False: \"escalier\"})\n",
    "    )\n",
    "\n",
    "    # Create 'annee_debut_panne'\n",
    "    df_processed[\"annee_debut_panne\"] = df_processed[\n",
    "        \"date_debut_panne\"\n",
    "    ].dt.year  # Works directly on datetime objects\n",
    "\n",
    "    # Calculate durations\n",
    "    durations = df_processed.apply(calculate_duration, axis=1, result_type=\"expand\")\n",
    "    df_processed[[\"duree_indispo\", \"jour_indispo\"]] = durations\n",
    "\n",
    "    # Clean 'ligne' column\n",
    "    df_processed[\"ligne\"] = (\n",
    "        df_processed[\"ligne\"].astype(str).str.replace(\"T1 \", \"T1\", regex=False)\n",
    "    )\n",
    "\n",
    "    # Extract 'id'\n",
    "    df_processed[\"id\"] = (\n",
    "        df_processed[\"num_equip\"].str.extract(r\"(\\d+)\", expand=False).astype(float)\n",
    "    )\n",
    "\n",
    "    # Convert dates back to string format DD/MM/YYYY *only for saving intermediate files*\n",
    "    df_save = df_processed.copy()\n",
    "    df_save[\"date_debut_panne\"] = df_save[\"date_debut_panne\"].dt.strftime(\"%d/%m/%Y\")\n",
    "    # Handle NaT in date_fin_panne before formatting\n",
    "    df_save[\"date_fin_panne\"] = (\n",
    "        df_save[\"date_fin_panne\"].dt.strftime(\"%d/%m/%Y\").fillna(\"\")\n",
    "    )\n",
    "\n",
    "    # --- Save Processed Files ---\n",
    "    try:\n",
    "        df_save.to_csv(output_csv_path, sep=\";\", index=False, encoding=\"utf-8-sig\")\n",
    "        logging.info(f\"Saved cleaned CSV: {output_csv_path}\")\n",
    "        df_save.to_excel(output_excel_path, index=False)\n",
    "        logging.info(f\"Saved cleaned Excel: {output_excel_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving output files for {output_base_name}: {e}\")\n",
    "\n",
    "    # Return the DataFrame with dates as datetime objects for merging\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bee1222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_process = [\n",
    "    {\n",
    "        \"input\": \"Points marquants maintenance 2024.xlsx\",\n",
    "        \"output\": \"points_marquants_24_clean\",\n",
    "        \"skiprows\": 0,\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Points marquants maintenance Janv 2025.xlsx\",\n",
    "        \"output\": \"points_marquants_janv_25_clean\",\n",
    "        \"skiprows\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Points marquants maintenance Fevrier 2025.xlsx\",\n",
    "        \"output\": \"points_marquants_fev_25_clean\",\n",
    "        \"skiprows\": 1,\n",
    "        \"sheet_name\": \"Fevr 25\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Points marquants maintenance Mars 2025.xlsx\",\n",
    "        \"output\": \"points_marquants_mars_25_clean\",\n",
    "        \"skiprows\": 1,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e990377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 14:32:02,770 - INFO - Processing file: Points marquants maintenance 2024.xlsx\n",
      "2025-05-06 14:32:03,092 - INFO - Saved cleaned CSV: c:\\Users\\VBO\\code\\asc_esc_consolide\\points_marquants\\points_marquants_24_clean.csv\n",
      "2025-05-06 14:32:03,153 - INFO - Saved cleaned Excel: c:\\Users\\VBO\\code\\asc_esc_consolide\\points_marquants\\points_marquants_24_clean.xlsx\n",
      "2025-05-06 14:32:03,153 - INFO - Processing file: Points marquants maintenance Janv 2025.xlsx\n",
      "c:\\Users\\VBO\\code\\asc_esc_consolide\\.conda\\Lib\\site-packages\\openpyxl\\worksheet\\_read_only.py:85: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  for idx, row in parser.parse():\n",
      "2025-05-06 14:32:03,187 - INFO - Saved cleaned CSV: c:\\Users\\VBO\\code\\asc_esc_consolide\\points_marquants\\points_marquants_janv_25_clean.csv\n",
      "2025-05-06 14:32:03,202 - INFO - Saved cleaned Excel: c:\\Users\\VBO\\code\\asc_esc_consolide\\points_marquants\\points_marquants_janv_25_clean.xlsx\n",
      "2025-05-06 14:32:03,202 - INFO - Processing file: Points marquants maintenance Fevrier 2025.xlsx\n",
      "c:\\Users\\VBO\\code\\asc_esc_consolide\\.conda\\Lib\\site-packages\\openpyxl\\worksheet\\_read_only.py:85: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  for idx, row in parser.parse():\n",
      "2025-05-06 14:32:03,240 - INFO - Saved cleaned CSV: c:\\Users\\VBO\\code\\asc_esc_consolide\\points_marquants\\points_marquants_fev_25_clean.csv\n",
      "2025-05-06 14:32:03,253 - INFO - Saved cleaned Excel: c:\\Users\\VBO\\code\\asc_esc_consolide\\points_marquants\\points_marquants_fev_25_clean.xlsx\n",
      "2025-05-06 14:32:03,253 - INFO - Processing file: Points marquants maintenance Mars 2025.xlsx\n",
      "c:\\Users\\VBO\\code\\asc_esc_consolide\\.conda\\Lib\\site-packages\\openpyxl\\worksheet\\_read_only.py:85: UserWarning: Conditional Formatting extension is not supported and will be removed\n",
      "  for idx, row in parser.parse():\n",
      "c:\\Users\\VBO\\code\\asc_esc_consolide\\.conda\\Lib\\site-packages\\openpyxl\\worksheet\\_read_only.py:85: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  for idx, row in parser.parse():\n",
      "2025-05-06 14:32:03,303 - INFO - Saved cleaned CSV: c:\\Users\\VBO\\code\\asc_esc_consolide\\points_marquants\\points_marquants_mars_25_clean.csv\n",
      "2025-05-06 14:32:03,325 - INFO - Saved cleaned Excel: c:\\Users\\VBO\\code\\asc_esc_consolide\\points_marquants\\points_marquants_mars_25_clean.xlsx\n"
     ]
    }
   ],
   "source": [
    "processed_dataframes = {}\n",
    "for file_info in files_to_process:\n",
    "    df = process_maintenance_file(\n",
    "        file_info[\"input\"],\n",
    "        file_info[\"output\"],\n",
    "        skiprows=file_info[\"skiprows\"],\n",
    "        sheet_name=file_info.get(\"sheet_name\", 0),  # Use .get() for optional keys\n",
    "    )\n",
    "    if df is not None:\n",
    "        processed_dataframes[file_info[\"output\"]] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "578ffbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the relevant dataframes for merging\n",
    "dfs_to_merge = []\n",
    "keys_to_merge = [\n",
    "    \"points_marquants_24_clean\",\n",
    "    \"points_marquants_janv_25_clean\",\n",
    "    \"points_marquants_fev_25_clean\",\n",
    "    \"points_marquants_mars_25_clean\",\n",
    "]\n",
    "for key in keys_to_merge:\n",
    "    if key in processed_dataframes:\n",
    "        dfs_to_merge.append(processed_dataframes[key])\n",
    "    else:\n",
    "        logging.warning(\n",
    "            f\"DataFrame '{key}' not found in processed data. It might have failed during processing.\"\n",
    "        )\n",
    "\n",
    "if not dfs_to_merge:\n",
    "    logging.error(\"No dataframes available for merging. Exiting.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe912382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the selected dataframes\n",
    "merged_df = pd.concat(dfs_to_merge, ignore_index=True)\n",
    "logging.info(\n",
    "    f\"Concatenated {len(dfs_to_merge)} dataframes. Total rows before deduplication: {len(merged_df)}\"\n",
    ")\n",
    "\n",
    "# Clean before deduplication\n",
    "merged_df.dropna(\n",
    "    subset=[\"date_debut_panne\", \"num_equip\"], inplace=True\n",
    ")  # Keep rows with essential keys\n",
    "merged_df.dropna(how=\"all\", inplace=True)  # Remove completely empty rows if any"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
